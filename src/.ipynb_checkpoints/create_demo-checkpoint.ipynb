{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,time,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import ops\n",
    "from resnet_tsm import resnet18 as resnet\n",
    "from utils import get_dtstr, emo2txt, imread_to_rgb, crop_img\n",
    "import face_recognition\n",
    "import moviepy.editor as mpe\n",
    "\n",
    "# restore net\n",
    "net = resnet().cuda()\n",
    "ckpt = torch.load(ops.weight_path+'/resnet18_tsm_weights.tar')\n",
    "net.load_state_dict(ckpt['model_state_dict'])\n",
    "net.eval()\n",
    "\n",
    "# episode_path\n",
    "DB_PATH = '/home/jhchoi/datasets2/friends/'\n",
    "f_b_size = 4\n",
    "\n",
    "# demo path\n",
    "DEMO_PATH = '../../demo/'\n",
    "\n",
    "def clear_cache(vid_imgs):\n",
    "    for f in vid_imgs:\n",
    "        os.remove(f)\n",
    "    print 'done'\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124"
     ]
    }
   ],
   "source": [
    "epi_sel = 'ep04'\n",
    "\n",
    "f_start_ms = (4,26)\n",
    "f_duration = 5\n",
    "\n",
    "f_start_sec = f_start_ms[0]*60 + f_start_ms[1]\n",
    "f_end_sec = f_start_sec + f_duration\n",
    "\n",
    "f_start = int(f_start_sec*23.976) - f_b_size\n",
    "f_end   = int(f_end_sec*23.976)\n",
    "\n",
    "vid_imgs = []\n",
    "f_buffer = []\n",
    "for i, f_i in enumerate(range(f_start, f_end)):\n",
    "    sys.stdout.write(\"\\r\"+str(i+1)+'/'+str(f_end-f_start))\n",
    "    f_fname = str('%05d'%f_i)+'.jpg'\n",
    "    \n",
    "    # wait frame buffer\n",
    "    f_img = imread_to_rgb(os.path.join(DB_PATH, epi_sel, f_fname))\n",
    "    f_buffer.append(f_img)\n",
    "    if i>f_b_size-1:\n",
    "        f_buffer.remove(f_buffer[0])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    # detect faces\n",
    "    faces_coor = np.array(face_recognition.face_locations((f_img*255).astype(np.uint8))).astype(int)\n",
    "    if faces_coor.ndim < 2:\n",
    "        # no face, continue\n",
    "        continue\n",
    "    else:\n",
    "        num_faces = faces_coor.shape[0]\n",
    "    # refine coordinates as (xmin,ymin,xmax,ymax)\n",
    "    faces_coor = np.array([faces_coor[:,3], faces_coor[:,0], faces_coor[:,1], faces_coor[:,2]])\n",
    "    \n",
    "    # draw bb for faces\n",
    "    f_img_bb = f_img.copy()\n",
    "    for j in range(num_faces):\n",
    "        coor = faces_coor[:,j]\n",
    "        f_img_bb = cv2.rectangle(f_img_bb, (coor[0], coor[1]), (coor[2], coor[3]), (1,1,0), 3)\n",
    "    #cv2.imwrite(os.path.join(DEMO_PATH,f_fname), (f_img_bb[:,:,[2,1,0]]*255).astype(np.uint8))\n",
    "    \n",
    "    # extract faces and get emotions\n",
    "    for j in range(num_faces):\n",
    "        # get face crops from buffer\n",
    "        coor = faces_coor[:,j]\n",
    "        w = coor[2] - coor[0]\n",
    "        h = coor[3] - coor[1]\n",
    "        m = 0.5\n",
    "        s = np.sqrt((w+(w+h)*m)*(h+(w+h)*m))\n",
    "\n",
    "        f_crops = [crop_img(im, int(coor[0]+w*0.5), int(coor[1]+h*0.5), int(s), int(s), True) for im in f_buffer]\n",
    "        f_batch = [cv2.resize(f_c, (224,224)) for f_c in f_crops]\n",
    "        # net forward - get emotion\n",
    "        f_batch = torch.Tensor(np.array(f_batch)).cuda()\n",
    "        f_batch = f_batch.unsqueeze(0).permute(0,1,4,2,3)\n",
    "        with torch.no_grad():\n",
    "            #f_emo = net(f_batch).mean(dim=1)[0].argmax().detach().cpu().numpy()\n",
    "            f_emo = net(f_batch)[0,-2:,:].mean(0).argmax().detach().cpu().numpy()\n",
    "        # write text\n",
    "        f_emo_txt =  emo2txt(f_emo)\n",
    "        font_size = 2\n",
    "        f_img_bb = cv2.putText(f_img_bb, f_emo_txt,(coor[0], coor[3]+font_size*12), cv2.FONT_HERSHEY_PLAIN, font_size, (1,1,0), 2)\n",
    "        \n",
    "    imf_name = os.path.join(DEMO_PATH,'temp',epi_sel+'_'+f_fname)\n",
    "    vid_imgs.append(imf_name)\n",
    "    cv2.imwrite(imf_name, (f_img_bb[:,:,[2,1,0]]*255).astype(np.uint8))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/117 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ../../demo/ep04_6373_6497.mp4.\n",
      "Moviepy - Writing video ../../demo/ep04_6373_6497.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ../../demo/ep04_6373_6497.mp4\n"
     ]
    }
   ],
   "source": [
    "vid_clip = mpe.ImageSequenceClip(vid_imgs, fps=6)\n",
    "vid_clip.write_videofile(os.path.join(DEMO_PATH, epi_sel+'_'+str(f_start)+'_'+str(f_end)+'.mp4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
